Проект spark job
Обработка файла (5-10 Гб) средствами Dataframe API. Файл хранится в локальной файловой системе.

1) понятная структура проекта
1) Описание через README.md
1) тесты
1) Предусмотреть конфигурирование джобы через файл конфига application.conf (библиотека pureConfig)+переменные окружения
  - имя джобы,
  - имя файла источника данных
  - имя файла с результатами обработки
  - изменение уровня логирования
1) Добавить дополнительное логирование(например вывести в лог конфиг, с которым запускалась джоба)
1) чистый код( + комментарии, логи, там где нужно)
1) организован сбор метрик и отображение в Grafana
1) docker-compose с необходимым окружением(spark-master, spark-worker, prometheus, grafana, бд и т.д.)
1) Обязательно использование функций join, groupBy, agg
1) Получить fat-jar файл джобы, для запуска на кластере с помощью spark-submit(плагин sbt-assembly)
